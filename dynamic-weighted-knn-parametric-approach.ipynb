{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22e64e7",
   "metadata": {
    "papermill": {
     "duration": 0.005573,
     "end_time": "2024-05-13T16:32:27.641071",
     "exception": false,
     "start_time": "2024-05-13T16:32:27.635498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## What is KNN?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, intuitive, and powerful machine learning technique used for classification and regression tasks. It belongs to the family of instance-based learning algorithms, also known as lazy learning algorithms, because it does not build an explicit model during the training phase. Instead, it memorizes the training dataset and makes predictions based on the most similar instances (neighbors) in the dataset.\n",
    "\n",
    "## How Does KNN Work?\n",
    "\n",
    "KNN operates based on the following steps:\n",
    "1. **Storing the Dataset**: During the training phase, KNN simply stores the training data points along with their corresponding labels.\n",
    "2. **Calculating Distance**: When a new data point needs to be classified, KNN calculates the distance between this new point and all the points in the training dataset. Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "3. **Finding Neighbors**: It then identifies the 'k' nearest neighbors to the new data point. The value of 'k' is a hyperparameter that determines the number of closest neighbors to consider.\n",
    "4. **Majority Voting**: For classification tasks, KNN assigns the most frequent label among the 'k' neighbors to the new data point. For regression tasks, it computes the average value of the 'k' neighbors.\n",
    "\n",
    "## Strengths of KNN\n",
    "\n",
    "- **Simplicity**: KNN is easy to understand and implement, making it an excellent choice for beginners in machine learning.\n",
    "- **No Training Phase**: Since KNN is a lazy learning algorithm, there is no explicit training phase, which can save computational resources.\n",
    "- **Flexibility**: KNN can be used for both classification and regression tasks.\n",
    "- **Adaptability**: KNN can naturally handle multi-class classification problems.\n",
    "\n",
    "## Weaknesses of KNN\n",
    "\n",
    "- **Computational Cost**: The prediction phase can be slow, especially with large datasets, as it involves calculating the distance between the new data point and all training points.\n",
    "- **Memory Usage**: KNN requires storing the entire training dataset, which can be memory-intensive.\n",
    "- **Sensitivity to Irrelevant Features**: KNN can be adversely affected by irrelevant or redundant features, which can distort distance calculations.\n",
    "- **Choice of 'k' and Distance Metric**: The performance of KNN heavily depends on the choice of 'k' and the distance metric used. An inappropriate choice can lead to poor performance.\n",
    "\n",
    "![K Nearest Neighbors illustration by GeeksforGeeks](https://media.geeksforgeeks.org/wp-content/uploads/20231214111348/K-Nearest-Neighbors-(1)-660.png \"Source: GeeksforGeeks\")\n",
    "\n",
    "## Enhancing KNN with Learnable Weights\n",
    "\n",
    "In this notebook, I introduce an enhancement to the traditional KNN algorithm by incorporating feature weights that are learned using gradient descent. This approach aims to address some of the weaknesses of standard KNN, particularly its sensitivity to irrelevant features. By assigning and optimizing weights for each feature, our Dynamic Weighted KNN algorithm adapts to the importance of different features, potentially improving classification accuracy and robustness. **We will examine whether this is worth implementing and what disadvantages it may offer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028c3ecf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:27.655789Z",
     "iopub.status.busy": "2024-05-13T16:32:27.655121Z",
     "iopub.status.idle": "2024-05-13T16:32:29.813288Z",
     "shell.execute_reply": "2024-05-13T16:32:29.811891Z"
    },
    "papermill": {
     "duration": 2.170451,
     "end_time": "2024-05-13T16:32:29.817977",
     "exception": false,
     "start_time": "2024-05-13T16:32:27.647526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2bd19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:29.840970Z",
     "iopub.status.busy": "2024-05-13T16:32:29.839909Z",
     "iopub.status.idle": "2024-05-13T16:32:29.862520Z",
     "shell.execute_reply": "2024-05-13T16:32:29.860484Z"
    },
    "papermill": {
     "duration": 0.042766,
     "end_time": "2024-05-13T16:32:29.868682",
     "exception": false,
     "start_time": "2024-05-13T16:32:29.825916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=750, n_features=5, \n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ccb6b",
   "metadata": {
    "papermill": {
     "duration": 0.007,
     "end_time": "2024-05-13T16:32:29.883078",
     "exception": false,
     "start_time": "2024-05-13T16:32:29.876078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dynamic Weighted KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c469f642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:29.902028Z",
     "iopub.status.busy": "2024-05-13T16:32:29.901250Z",
     "iopub.status.idle": "2024-05-13T16:32:29.937265Z",
     "shell.execute_reply": "2024-05-13T16:32:29.934957Z"
    },
    "papermill": {
     "duration": 0.05357,
     "end_time": "2024-05-13T16:32:29.943740",
     "exception": false,
     "start_time": "2024-05-13T16:32:29.890170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicWeightedKNN:\n",
    "    def __init__(self, k=3, learning_rate=0.01):\n",
    "        \"\"\"Initialize the DynamicWeightedKNN with basic parameters.\"\"\"\n",
    "        self.k = k  # Number of neighbors to consider\n",
    "        self.learning_rate = learning_rate  # Learning rate for weight updates\n",
    "        self.X_train = None  # Training features\n",
    "        self.y_train = None  # Training labels\n",
    "        self.weights = None  # Weights for the features\n",
    "    \n",
    "    def fit(self, X, y, weights=None, n_iterations=25):\n",
    "        \"\"\"Fit the model using the provided training dataset and optional initial weights.\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.weights = np.ones(X.shape[1]) if weights is None else weights\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            self.update_weights()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict the probabilities for the dataset X using the weighted Euclidean distance.\"\"\"\n",
    "        probas = []\n",
    "        for x in X:\n",
    "            distances = np.array([self.weighted_euclidean_distance(x, x_train) for x_train in self.X_train])\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            prob = np.mean(k_nearest_labels)\n",
    "            probas.append(prob)\n",
    "        return np.array(probas)\n",
    "    \n",
    "    def update_weights(self):\n",
    "        \"\"\"Update weights using the gradient of the loss function incorporating logistic derivative.\"\"\"\n",
    "        y_pred = self.predict_proba(self.X_train)\n",
    "        error = y_pred - self.y_train\n",
    "        for i in range(len(self.weights)):\n",
    "            # Incorporating the logistic derivative for the gradient calculation\n",
    "            gradient = np.dot(error * y_pred * (1 - y_pred), self.X_train[:, i])\n",
    "            self.weights[i] -= self.learning_rate * gradient\n",
    "        self.weights = np.maximum(0, self.weights)  # Ensure weights remain non-negative\n",
    "    \n",
    "    def weighted_euclidean_distance(self, x, y):\n",
    "        \"\"\"Calculate the weighted Euclidean distance between two vectors.\"\"\"\n",
    "        difference = x - y\n",
    "        weighted_squared_diff = self.weights * np.square(difference)\n",
    "        return np.sqrt(np.sum(weighted_squared_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e91fdb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:29.964585Z",
     "iopub.status.busy": "2024-05-13T16:32:29.963906Z",
     "iopub.status.idle": "2024-05-13T16:32:29.981504Z",
     "shell.execute_reply": "2024-05-13T16:32:29.979813Z"
    },
    "papermill": {
     "duration": 0.032189,
     "end_time": "2024-05-13T16:32:29.984700",
     "exception": false,
     "start_time": "2024-05-13T16:32:29.952511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"Initialize the BasicKNN with the number of neighbors to consider.\"\"\"\n",
    "        self.k = k\n",
    "        self.X_train = None  # Training features\n",
    "        self.y_train = None  # Training labels\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using the provided training dataset.\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the labels for the dataset X.\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = np.array([self.euclidean_distance(x, x_train) for x_train in self.X_train])\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            majority_vote = np.argmax(np.bincount(k_nearest_labels))\n",
    "            predictions.append(majority_vote)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def euclidean_distance(self, x, y):\n",
    "        \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n",
    "        return np.sqrt(np.sum((x - y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c481d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:29.997515Z",
     "iopub.status.busy": "2024-05-13T16:32:29.996916Z",
     "iopub.status.idle": "2024-05-13T16:32:30.011919Z",
     "shell.execute_reply": "2024-05-13T16:32:30.009861Z"
    },
    "papermill": {
     "duration": 0.025418,
     "end_time": "2024-05-13T16:32:30.015439",
     "exception": false,
     "start_time": "2024-05-13T16:32:29.990021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Prepare lists to store results\n",
    "standard_knn_accuracies = []\n",
    "dynamic_weighted_knn_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89775ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:30.029428Z",
     "iopub.status.busy": "2024-05-13T16:32:30.028806Z",
     "iopub.status.idle": "2024-05-13T16:32:35.723044Z",
     "shell.execute_reply": "2024-05-13T16:32:35.720899Z"
    },
    "papermill": {
     "duration": 5.706395,
     "end_time": "2024-05-13T16:32:35.727670",
     "exception": false,
     "start_time": "2024-05-13T16:32:30.021275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.68037223815918 seconds\n",
      "Average Accuracy with Standard KNN: 0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Conduct Stratified K-Fold Cross-Validation for Standard KNN\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Standard KNN\n",
    "    standard_knn = KNN(k=5)\n",
    "    standard_knn.fit(X_train, y_train)\n",
    "    standard_predictions = standard_knn.predict(X_test)\n",
    "    standard_accuracy = accuracy_score(y_test, standard_predictions)\n",
    "    standard_knn_accuracies.append(standard_accuracy)\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    \n",
    "# Print average accuracy for Standard KNN\n",
    "print(\"Average Accuracy with Standard KNN:\", np.mean(standard_knn_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f2097e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T16:32:35.742144Z",
     "iopub.status.busy": "2024-05-13T16:32:35.741657Z",
     "iopub.status.idle": "2024-05-13T16:42:37.470584Z",
     "shell.execute_reply": "2024-05-13T16:42:37.469564Z"
    },
    "papermill": {
     "duration": 601.743896,
     "end_time": "2024-05-13T16:42:37.478390",
     "exception": false,
     "start_time": "2024-05-13T16:32:35.734494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 601.7196817398071 seconds\n",
      "Average Accuracy with Dynamic Weighted KNN: 0.9306666666666666\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Conduct Stratified K-Fold Cross-Validation for Dynamic Weighted KNN\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Dynamic Weighted KNN\n",
    "    dynamic_knn = DynamicWeightedKNN(k=5, learning_rate=0.01)\n",
    "    dynamic_knn.fit(X_train, y_train, n_iterations=25)\n",
    "    dynamic_predictions = dynamic_knn.predict_proba(X_test) > 0.5  # Thresholding to get binary predictions\n",
    "    dynamic_accuracy = accuracy_score(y_test, dynamic_predictions)\n",
    "    dynamic_weighted_knn_accuracies.append(dynamic_accuracy)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "# Print average accuracy for Dynamic Weighted KNN\n",
    "print(\"Average Accuracy with Dynamic Weighted KNN:\", np.mean(dynamic_weighted_knn_accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278dec9",
   "metadata": {
    "papermill": {
     "duration": 0.004986,
     "end_time": "2024-05-13T16:42:37.488360",
     "exception": false,
     "start_time": "2024-05-13T16:42:37.483374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Outcomes\n",
    "\n",
    "### Performance Analysis\n",
    "\n",
    "After applying both the Dynamic Weighted KNN and the standard KNN to our selected dataset using stratified cross-validation, we observed key differences in both accuracy and computational efficiency.\n",
    "\n",
    "**Accuracy**: The Dynamic Weighted KNN showed mixed results in terms of accuracy compared to the standard KNN. While it demonstrated slightly better accuracy on some datasets, it also performed slightly worse on others. This variability suggests that the introduction of feature-specific weights, optimized through gradient descent, might not consistently capture the importance of different features across diverse datasets.\n",
    "\n",
    "**Speed**: The Dynamic Weighted KNN was significantly slower than the standard KNN. The added computational overhead from updating weights using gradient descent at each training step substantially increases the time required for the model to train. This drawback makes the Dynamic Weighted KNN impractical for scenarios where quick training or real-time prediction is crucial.\n",
    "\n",
    "### Considerations for Slowness\n",
    "\n",
    "The slower performance of the Dynamic Weighted KNN is a significant limitation. Even with occasional accuracy improvements, its computational inefficiency could be a dealbreaker in many real-world applications where speed and scalability are paramount.\n",
    "\n",
    "### Potential Optimizations\n",
    "\n",
    "Despite its current limitations, there are several strategies that could potentially optimize the performance of the Dynamic Weighted KNN:\n",
    "\n",
    "- **Efficient Data Structures**: Using more efficient data structures like KD-Trees or Ball Trees could reduce the computational complexity associated with finding the nearest neighbors, thus speeding up both the training and prediction phases.\n",
    "- **Parallel Processing**: Leveraging parallel processing techniques to distribute the computation of distances and updates across multiple cores or GPUs could significantly reduce execution time.\n",
    "- **Feature Selection**: Performing feature selection before training to reduce the dimensionality of the data could decrease the number of weights that need to be optimized, thereby simplifying the model and speeding up the calculations.\n",
    "- **Algorithmic Refinements**: Applying more sophisticated methods for gradient descent (e.g., incorporating momentum or adaptive learning rate techniques) could improve the efficiency of weight updates.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Given the variable accuracy and significantly slower performance of the Dynamic Weighted KNN, it is not recommended for use over the standard KNN in most practical scenarios. The potential improvements in accuracy do not outweigh the considerable increase in computational costs. However, with targeted optimizations and enhancements, there may be potential to make this model viable for specific applications where the unique advantages of feature-weighted learning can be fully realized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d79e5",
   "metadata": {
    "papermill": {
     "duration": 0.004716,
     "end_time": "2024-05-13T16:42:37.498181",
     "exception": false,
     "start_time": "2024-05-13T16:42:37.493465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Time Complexity\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "During the training phase:\n",
    "- **Distance Calculation**: Calculating the weighted Euclidean distance between all pairs of data points involves O(d) operations per pair. Given n data points, computing distances for all pairs leads to O(n^2 * d) complexity.\n",
    "- **Weight Updates**: The gradient update for weights based on the calculated distances and errors involves O(n * d) operations per iteration.\n",
    "- **Iterations**: Repeating the weight update process across t iterations, the total complexity becomes O(t * n^2 * d)."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 614.780358,
   "end_time": "2024-05-13T16:42:38.129556",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-13T16:32:23.349198",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
