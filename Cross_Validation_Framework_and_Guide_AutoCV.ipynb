{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d409dc3e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007702,
     "end_time": "2024-05-21T11:39:03.343401",
     "exception": false,
     "start_time": "2024-05-21T11:39:03.335699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is Cross Validation?\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the skill of machine learning models. It involves partitioning a dataset into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). This process is repeated multiple times (folds) to ensure that the model's performance is consistent and not due to random chance.\n",
    "\n",
    "## Why Do We Need Cross Validation?\n",
    "\n",
    "1. **Model Evaluation**: It provides a more reliable estimate of a model's performance than a single train/test split, especially on small datasets.\n",
    "   \n",
    "2. **Model Selection**: Helps in selecting the best model among different types by comparing their performance.\n",
    "\n",
    "3. **Parameter Tuning**: Assists in tuning hyperparameters of a model to achieve the best performance.\n",
    "\n",
    "4. **Detect Overfitting**: It helps to identify if the model is overfitting to the training data and performs poorly on unseen data.\n",
    "\n",
    "5. **Bias-Variance Tradeoff**: It provides a balance between bias and variance, leading to a more generalized model.\n",
    "\n",
    "## When Do We Need Cross Validation?\n",
    "\n",
    "1. **Small Datasets**: When the dataset is not large enough, cross-validation maximizes the amount of data used for training and testing.\n",
    "   \n",
    "2. **Model Comparison**: When you need to compare multiple machine learning models to find the best one for your data.\n",
    "\n",
    "3. **Hyperparameter Tuning**: During the process of finding the optimal hyperparameters for a model.\n",
    "\n",
    "4. **Generalization**: When you want to ensure that your model generalizes well to unseen data.\n",
    "\n",
    "## When Do We Not Need Cross Validation?\n",
    "\n",
    "1. **Large Datasets**: When the dataset is large enough that a single train/test split can give a reliable estimate of model performance.\n",
    "   \n",
    "2. **Real-Time Predictions**: In real-time or streaming applications where splitting data into folds and training multiple models is impractical due to time constraints.\n",
    "\n",
    "3. **Data Leakage**: If there's a risk of data leakage between folds, which can happen if the data is not properly randomized or if there's temporal dependency.\n",
    "\n",
    "4. **Simplistic Models**: When using very simple models or in cases where the problem is straightforward and does not require extensive validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604675a1",
   "metadata": {
    "papermill": {
     "duration": 0.007777,
     "end_time": "2024-05-21T11:39:03.359045",
     "exception": false,
     "start_time": "2024-05-21T11:39:03.351268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Types of Cross Validation in Scikit-Learn\n",
    "Cross-validation is a key technique for assessing the performance and robustness of a machine learning model. **Scikit-learn** offers several methods for cross-validation, each suited to different types of data and use cases. Here is a detailed overview of the various cross-validation techniques provided by scikit-learn:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://i.ibb.co/2qxCG23/cv.gif\" alt=\"K-Fold Cross Validation source: 0xkerem\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "</div>\n",
    "\n",
    "## 1. K-Fold Cross Validation\n",
    "\n",
    "K-Fold Cross Validation is a robust method used to evaluate the performance of a machine learning model. It involves dividing the dataset into `k` equal-sized subsets or \"folds.\" The model is trained `k` times, each time using a different fold as the validation set and the remaining folds as the training set. The final performance metric is the average of the metrics calculated for each fold. This method helps to ensure that the model's performance is not dependent on the specific partitioning of the dataset.\n",
    "\n",
    "#### How K-Fold Cross Validation Works\n",
    "1. **Divide the Data**: Split the data into `k` equal-sized folds.\n",
    "2. **Train and Validate**: For each fold:\n",
    "   - Train the model on `k-1` folds.\n",
    "   - Validate the model on the remaining fold.\n",
    "3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n",
    "4. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n",
    "\n",
    "![K-Fold Cross Validation source: Towards Data Science](https://i.ibb.co/H7mLgkk/k-fold-cv.png)\n",
    "\n",
    "### When to Use K-Fold Cross Validation\n",
    "- **Small Datasets**: When you have limited data, K-Fold Cross Validation allows you to make the most of your data by ensuring each data point is used for both training and validation.\n",
    "- **Model Selection**: When selecting the best model from a set of candidate models, K-Fold Cross Validation provides a reliable estimate of each modelâ€™s performance.\n",
    "- **Hyperparameter Tuning**: It helps in tuning hyperparameters by providing a robust evaluation metric.\n",
    "- **Avoiding Overfitting**: It helps in detecting overfitting by ensuring that the model performs well on different subsets of the data.\n",
    "\n",
    "### Using K-Fold Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a convenient way to implement K-Fold Cross Validation using the `KFold` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `n_splits`: Number of folds.\n",
    "- `shuffle`: Whether to shuffle the data before splitting into folds.\n",
    "- `random_state`: Seed for random number generator to ensure reproducibility.\n",
    "\n",
    "### Advantages\n",
    "- **Robust Evaluation**: Provides a more reliable estimate of model performance.\n",
    "- **Efficient Use of Data**: Makes efficient use of limited data by utilizing every observation for both training and validation.\n",
    "- **Reduced Variance**: Reduces the variance of performance metrics compared to a single train-test split.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Cost**: More computationally expensive than a single train-test split, especially for large datasets.\n",
    "- **Model Complexity**: May lead to more complex models due to the multiple training processes.\n",
    "\n",
    "By using K-Fold Cross Validation, you can ensure a thorough and reliable evaluation of your machine learning models, leading to better model selection and improved generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe8d5d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:39:03.378266Z",
     "iopub.status.busy": "2024-05-21T11:39:03.377532Z",
     "iopub.status.idle": "2024-05-21T11:39:05.291236Z",
     "shell.execute_reply": "2024-05-21T11:39:05.289669Z"
    },
    "papermill": {
     "duration": 1.927371,
     "end_time": "2024-05-21T11:39:05.293971",
     "exception": false,
     "start_time": "2024-05-21T11:39:03.366600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: [1.         1.         0.93333333 0.96666667 0.96666667]\n",
      "Mean accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define K-Fold Cross Validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fcc61",
   "metadata": {
    "papermill": {
     "duration": 0.006876,
     "end_time": "2024-05-21T11:39:05.308165",
     "exception": false,
     "start_time": "2024-05-21T11:39:05.301289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Stratified K-Fold Cross Validation\n",
    "\n",
    "Stratified K-Fold Cross Validation is a variation of K-Fold Cross Validation that ensures each fold is representative of the entire dataset by maintaining the same proportion of each class label. This method is particularly useful when dealing with imbalanced datasets, where some classes are underrepresented.\n",
    "\n",
    "#### How Stratified K-Fold Cross Validation Works\n",
    "1. **Divide the Data**: Split the data into `k` equal-sized folds while preserving the class distribution in each fold.\n",
    "2. **Train and Validate**: For each fold:\n",
    "   - Train the model on `k-1` folds.\n",
    "   - Validate the model on the remaining fold.\n",
    "3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n",
    "4. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n",
    "\n",
    "### When to Use Stratified K-Fold Cross Validation\n",
    "- **Imbalanced Datasets**: When dealing with imbalanced datasets, Stratified K-Fold Cross Validation ensures that each fold is representative of the overall class distribution, providing a more reliable evaluation.\n",
    "- **Classification Problems**: Especially useful in classification problems where maintaining the class distribution in training and validation sets is crucial for model performance.\n",
    "- **Model Selection and Hyperparameter Tuning**: Helps in selecting the best model and tuning hyperparameters by providing a robust evaluation metric that accounts for class imbalance.\n",
    "\n",
    "### Using Stratified K-Fold Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a convenient way to implement Stratified K-Fold Cross Validation using the `StratifiedKFold` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `n_splits`: Number of folds.\n",
    "- `shuffle`: Whether to shuffle the data before splitting into folds.\n",
    "- `random_state`: Seed for random number generator to ensure reproducibility.\n",
    "\n",
    "### Advantages\n",
    "- **Class Distribution Preservation**: Maintains the proportion of each class in all folds, leading to more reliable performance metrics.\n",
    "- **Effective for Imbalanced Data**: Provides a better evaluation for models trained on imbalanced datasets.\n",
    "- **Reduced Bias**: Reduces bias in performance metrics by ensuring each fold is representative of the entire dataset.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Cost**: More computationally expensive than a single train-test split, especially for large datasets.\n",
    "- **Complexity in Implementation**: Slightly more complex to implement compared to standard K-Fold Cross Validation, although libraries like Scikit-learn simplify this process.\n",
    "\n",
    "Stratified K-Fold Cross Validation is a powerful tool for evaluating machine learning models, particularly when dealing with imbalanced datasets. It ensures that each fold is representative of the overall class distribution, leading to more reliable and robust performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666329ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:39:05.326008Z",
     "iopub.status.busy": "2024-05-21T11:39:05.325557Z",
     "iopub.status.idle": "2024-05-21T11:39:05.467121Z",
     "shell.execute_reply": "2024-05-21T11:39:05.465709Z"
    },
    "papermill": {
     "duration": 0.154853,
     "end_time": "2024-05-21T11:39:05.470754",
     "exception": false,
     "start_time": "2024-05-21T11:39:05.315901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
      "Mean accuracy: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define Stratified K-Fold Cross Validator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f30e23",
   "metadata": {
    "papermill": {
     "duration": 0.007899,
     "end_time": "2024-05-21T11:39:05.486679",
     "exception": false,
     "start_time": "2024-05-21T11:39:05.478780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Leave-One-Out Cross Validation (LOOCV)\n",
    "\n",
    "Leave-One-Out Cross Validation (LOOCV) is an extreme case of K-Fold Cross Validation where `k` equals the number of data points in the dataset. In LOOCV, each data point is used once as a validation set while the remaining data points form the training set. This process is repeated for each data point, and the performance metric is averaged across all iterations.\n",
    "\n",
    "#### How Leave-One-Out Cross Validation Works\n",
    "1. **Divide the Data**: Treat each data point as a single fold.\n",
    "2. **Train and Validate**: For each data point:\n",
    "   - Train the model on the remaining `n-1` data points.\n",
    "   - Validate the model on the single data point.\n",
    "3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each iteration.\n",
    "4. **Average Metrics**: Average the performance metrics across all iterations to obtain a final evaluation.\n",
    "\n",
    "### When to Use Leave-One-Out Cross Validation\n",
    "- **Small Datasets**: Ideal for very small datasets where splitting the data into larger folds is not feasible.\n",
    "- **High-Variance Models**: Useful when you want to get an unbiased estimate of model performance, though it may have high variance.\n",
    "- **Model Evaluation**: Provides a thorough evaluation as each data point is used for validation exactly once.\n",
    "\n",
    "### Using Leave-One-Out Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a convenient way to implement LOOCV using the `LeaveOneOut` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `cv`: Number of folds, which in the case of LOOCV is equal to the number of data points in the dataset.\n",
    "\n",
    "### Advantages\n",
    "- **Unbiased Estimate**: Provides an unbiased estimate of the modelâ€™s performance since each data point is used for validation exactly once.\n",
    "- **Maximal Data Utilization**: Ensures maximal utilization of data for training since `n-1` data points are used for training in each iteration.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computationally Intensive**: Very computationally expensive, especially for large datasets, as it requires training the model `n` times.\n",
    "- **High Variance**: The performance metric can have high variance since each validation set contains only one data point.\n",
    "\n",
    "Leave-One-Out Cross Validation is a thorough and unbiased method for model evaluation, particularly useful for small datasets. However, its high computational cost makes it impractical for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d0e7c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:39:05.504789Z",
     "iopub.status.busy": "2024-05-21T11:39:05.504359Z",
     "iopub.status.idle": "2024-05-21T11:39:09.357604Z",
     "shell.execute_reply": "2024-05-21T11:39:09.356084Z"
    },
    "papermill": {
     "duration": 3.86565,
     "end_time": "2024-05-21T11:39:09.360247",
     "exception": false,
     "start_time": "2024-05-21T11:39:05.494597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Mean accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define Leave-One-Out Cross Validator\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=loo, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d082364",
   "metadata": {
    "papermill": {
     "duration": 0.007845,
     "end_time": "2024-05-21T11:39:09.376034",
     "exception": false,
     "start_time": "2024-05-21T11:39:09.368189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Leave-P-Out Cross Validation (LPOCV)\n",
    "\n",
    "Leave-P-Out Cross Validation (LPOCV) is a generalization of Leave-One-Out Cross Validation (LOOCV). In LPOCV, `p` data points are left out for validation, and the model is trained on the remaining `n-p` data points. This process is repeated for all possible combinations of `p` data points. LPOCV provides a comprehensive evaluation of the model's performance but is computationally expensive, especially for large `p` and datasets.\n",
    "\n",
    "#### How Leave-P-Out Cross Validation Works\n",
    "1. **Divide the Data**: Generate all possible combinations of `p` data points to be used as validation sets.\n",
    "2. **Train and Validate**: For each combination:\n",
    "   - Train the model on the remaining `n-p` data points.\n",
    "   - Validate the model on the `p` data points.\n",
    "3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each iteration.\n",
    "4. **Average Metrics**: Average the performance metrics across all iterations to obtain a final evaluation.\n",
    "\n",
    "### When to Use Leave-P-Out Cross Validation\n",
    "- **Small to Medium Datasets**: Feasible for smaller datasets where the number of combinations is manageable.\n",
    "- **Thorough Evaluation**: Provides a thorough and exhaustive evaluation of model performance by considering all possible validation sets of size `p`.\n",
    "- **Model Evaluation**: Useful for understanding model performance across different subsets of the data.\n",
    "\n",
    "### Using Leave-P-Out Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a way to implement LPOCV using the `LeavePOut` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `p`: Number of data points to leave out for validation.\n",
    "- `cv`: Number of combinations, which is determined by the number of ways to choose `p` data points from `n`.\n",
    "\n",
    "### Advantages\n",
    "- **Comprehensive Evaluation**: Provides a thorough and exhaustive evaluation of the model by considering all possible subsets of size `p`.\n",
    "- **Detailed Insight**: Offers detailed insight into model performance across different combinations of data points.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computationally Intensive**: Very computationally expensive, especially for large datasets and larger values of `p`, due to the combinatorial explosion of possible subsets.\n",
    "- **High Complexity**: High complexity in implementation and computation makes it impractical for large datasets or large `p`.\n",
    "\n",
    "Leave-P-Out Cross Validation is a powerful tool for thorough model evaluation, especially useful for smaller datasets where an exhaustive assessment is feasible. Its computational intensity makes it less suitable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f24ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:39:09.393876Z",
     "iopub.status.busy": "2024-05-21T11:39:09.393155Z",
     "iopub.status.idle": "2024-05-21T11:44:01.672125Z",
     "shell.execute_reply": "2024-05-21T11:44:01.670160Z"
    },
    "papermill": {
     "duration": 292.298406,
     "end_time": "2024-05-21T11:44:01.682262",
     "exception": false,
     "start_time": "2024-05-21T11:39:09.383856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: [1. 1. 1. ... 1. 1. 1.]\n",
      "Mean accuracy: 0.965413870246085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define Leave-P-Out Cross Validator\n",
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=lpo, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49bea23",
   "metadata": {
    "papermill": {
     "duration": 0.007732,
     "end_time": "2024-05-21T11:44:01.698177",
     "exception": false,
     "start_time": "2024-05-21T11:44:01.690445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. ShuffleSplit Cross Validation\n",
    "\n",
    "ShuffleSplit Cross Validation is a method that involves randomly shuffling the dataset and then splitting it into a specified number of training and validation sets. Unlike K-Fold Cross Validation, ShuffleSplit does not ensure that each sample is used exactly once for validation. Instead, it allows for random sampling with replacement, which can be useful for generating multiple different training and validation splits.\n",
    "\n",
    "#### How ShuffleSplit Cross Validation Works\n",
    "1. **Shuffle the Data**: Randomly shuffle the dataset.\n",
    "2. **Split the Data**: Split the shuffled data into training and validation sets according to specified proportions.\n",
    "3. **Train and Validate**: For each split:\n",
    "   - Train the model on the training set.\n",
    "   - Validate the model on the validation set.\n",
    "4. **Repeat**: Repeat the process for a specified number of iterations.\n",
    "5. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each split.\n",
    "6. **Average Metrics**: Average the performance metrics across all splits to obtain a final evaluation.\n",
    "\n",
    "### When to Use ShuffleSplit Cross Validation\n",
    "- **Large Datasets**: Suitable for large datasets where traditional K-Fold Cross Validation may be computationally expensive.\n",
    "- **Random Sampling**: When you want to ensure that different random samples of the dataset are used for training and validation.\n",
    "- **Model Robustness**: Useful for testing the robustness of the model against different random splits of the data.\n",
    "\n",
    "### Using ShuffleSplit Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a convenient way to implement ShuffleSplit Cross Validation using the `ShuffleSplit` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `n_splits`: Number of re-shuffling and splitting iterations.\n",
    "- `test_size`: Proportion of the dataset to include in the validation set.\n",
    "- `train_size`: Proportion of the dataset to include in the training set (if not specified, it's the complement of `test_size`).\n",
    "- `random_state`: Seed for random number generator to ensure reproducibility.\n",
    "\n",
    "### Advantages\n",
    "- **Flexibility**: Allows for flexible training and validation sizes, making it adaptable to various dataset sizes and requirements.\n",
    "- **Random Sampling**: Provides different random splits of the data, which can help in assessing the robustness of the model.\n",
    "- **Computational Efficiency**: Can be more computationally efficient than K-Fold Cross Validation for large datasets.\n",
    "\n",
    "### Disadvantages\n",
    "- **Potential Overlap**: Some samples may be included in both training and validation sets across different splits, which may lead to less independent evaluation.\n",
    "- **Less Comprehensive**: Does not ensure that each sample is used exactly once for validation, potentially leading to less comprehensive evaluation compared to K-Fold Cross Validation.\n",
    "\n",
    "ShuffleSplit Cross Validation is a versatile and flexible method for evaluating machine learning models, particularly useful for large datasets and scenarios where random sampling is desired. Its ability to provide multiple different training and validation splits can help in assessing the robustness and generalization capability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2263c14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:44:01.715708Z",
     "iopub.status.busy": "2024-05-21T11:44:01.715238Z",
     "iopub.status.idle": "2024-05-21T11:44:01.994520Z",
     "shell.execute_reply": "2024-05-21T11:44:01.992666Z"
    },
    "papermill": {
     "duration": 0.291726,
     "end_time": "2024-05-21T11:44:01.997483",
     "exception": false,
     "start_time": "2024-05-21T11:44:01.705757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each split: [1.         0.96666667 0.96666667 0.93333333 0.93333333 1.\n",
      " 0.93333333 0.96666667 1.         0.9       ]\n",
      "Mean accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define ShuffleSplit Cross Validator\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=ss, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each split: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747fcf9",
   "metadata": {
    "papermill": {
     "duration": 0.007816,
     "end_time": "2024-05-21T11:44:02.014644",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.006828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Stratified ShuffleSplit Cross Validation\n",
    "\n",
    "Stratified ShuffleSplit Cross Validation is a variation of ShuffleSplit Cross Validation that maintains the proportion of each class in both the training and validation sets. This method is particularly useful for imbalanced datasets where it is important to ensure that each split preserves the original class distribution.\n",
    "\n",
    "#### How Stratified ShuffleSplit Cross Validation Works\n",
    "1. **Shuffle the Data**: Randomly shuffle the dataset while maintaining class distribution.\n",
    "2. **Split the Data**: Split the shuffled data into training and validation sets according to specified proportions, ensuring each set retains the class distribution.\n",
    "3. **Train and Validate**: For each split:\n",
    "   - Train the model on the training set.\n",
    "   - Validate the model on the validation set.\n",
    "4. **Repeat**: Repeat the process for a specified number of iterations.\n",
    "5. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each split.\n",
    "6. **Average Metrics**: Average the performance metrics across all splits to obtain a final evaluation.\n",
    "\n",
    "### When to Use Stratified ShuffleSplit Cross Validation\n",
    "- **Imbalanced Datasets**: Particularly useful when dealing with imbalanced datasets to ensure each class is appropriately represented in training and validation sets.\n",
    "- **Classification Problems**: Ensures that the class distribution is preserved, leading to more reliable evaluation metrics.\n",
    "- **Model Robustness**: Useful for testing the robustness of the model against different random splits of the data while maintaining class proportions.\n",
    "\n",
    "### Using Stratified ShuffleSplit Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a convenient way to implement Stratified ShuffleSplit Cross Validation using the `StratifiedShuffleSplit` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `n_splits`: Number of re-shuffling and splitting iterations.\n",
    "- `test_size`: Proportion of the dataset to include in the validation set.\n",
    "- `train_size`: Proportion of the dataset to include in the training set (if not specified, it's the complement of `test_size`).\n",
    "- `random_state`: Seed for random number generator to ensure reproducibility.\n",
    "\n",
    "### Advantages\n",
    "- **Class Distribution Preservation**: Maintains the class distribution in each split, leading to more reliable and representative performance metrics.\n",
    "- **Random Sampling**: Provides different random splits of the data while preserving class proportions, useful for assessing model robustness.\n",
    "- **Flexible and Robust**: Offers the flexibility of ShuffleSplit with the added benefit of stratification, making it suitable for a wide range of dataset sizes and types.\n",
    "\n",
    "### Disadvantages\n",
    "- **Potential Overlap**: Some samples may be included in both training and validation sets across different splits, which may lead to less independent evaluation.\n",
    "- **Computational Cost**: Can be computationally intensive for very large datasets, though generally more efficient than exhaustive methods like LPOCV.\n",
    "\n",
    "Stratified ShuffleSplit Cross Validation combines the benefits of random sampling with the need to maintain class distributions, making it a powerful tool for evaluating machine learning models, especially on imbalanced datasets. This method helps ensure that evaluation metrics are reliable and representative of the model's performance across different splits of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df31833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:44:02.033080Z",
     "iopub.status.busy": "2024-05-21T11:44:02.032542Z",
     "iopub.status.idle": "2024-05-21T11:44:02.273744Z",
     "shell.execute_reply": "2024-05-21T11:44:02.272218Z"
    },
    "papermill": {
     "duration": 0.254185,
     "end_time": "2024-05-21T11:44:02.276695",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.022510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each split: [0.96666667 0.96666667 0.96666667 0.96666667 0.93333333 0.96666667\n",
      " 1.         0.96666667 0.93333333 0.96666667]\n",
      "Mean accuracy: 0.9633333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define Stratified ShuffleSplit Cross Validator\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=sss, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each split: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f3d95",
   "metadata": {
    "papermill": {
     "duration": 0.007977,
     "end_time": "2024-05-21T11:44:02.292665",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.284688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Group K-Fold Cross Validation\n",
    "\n",
    "Group K-Fold Cross Validation is a variation of K-Fold Cross Validation used when the data is organized into groups that should not be split across training and validation sets. It ensures that the same group is not represented in both training and validation sets, making it useful for scenarios where there is a need to avoid data leakage between groups, such as in time-series data, clustered data, or repeated measures.\n",
    "\n",
    "#### How Group K-Fold Cross Validation Works\n",
    "1. **Identify Groups**: Identify the groups within the dataset.\n",
    "2. **Divide Groups**: Split the groups into `k` folds while ensuring that all data points within a group remain in the same fold.\n",
    "3. **Train and Validate**: For each fold:\n",
    "   - Train the model on `k-1` folds.\n",
    "   - Validate the model on the remaining fold.\n",
    "4. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n",
    "5. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n",
    "\n",
    "### When to Use Group K-Fold Cross Validation\n",
    "- **Grouped Data**: When the dataset contains groups of data points that should not be split across training and validation sets to prevent data leakage.\n",
    "- **Time-Series Data**: When working with time-series data or sequential data where the order and grouping of data points are crucial.\n",
    "- **Hierarchical Data**: When dealing with hierarchical data or repeated measures where individual samples are not independent.\n",
    "\n",
    "### Using Group K-Fold Cross Validation in Scikit-Learn\n",
    "\n",
    "Scikit-learn provides a convenient way to implement Group K-Fold Cross Validation using the `GroupKFold` class and `cross_val_score` function.\n",
    "\n",
    "### Key Parameters\n",
    "- `n_splits`: Number of folds.\n",
    "- `groups`: Array-like structure containing group labels for the samples.\n",
    "\n",
    "### Advantages\n",
    "- **Prevents Data Leakage**: Ensures that data from the same group does not appear in both training and validation sets, preventing data leakage.\n",
    "- **Appropriate for Grouped Data**: Suitable for datasets with natural groupings, such as clustered data or repeated measures.\n",
    "- **Maintains Group Integrity**: Ensures the integrity of groups within the dataset, making it a robust evaluation method for grouped data.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Cost**: Slightly more computationally intensive than standard K-Fold Cross Validation due to the need to maintain group integrity.\n",
    "- **Complexity in Implementation**: Requires additional handling of group labels and careful management of group splits.\n",
    "\n",
    "Group K-Fold Cross Validation is a powerful tool for evaluating machine learning models when dealing with grouped data. It helps prevent data leakage and ensures that the evaluation metrics are reliable and representative of the model's performance across different groups in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2425649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:44:02.310899Z",
     "iopub.status.busy": "2024-05-21T11:44:02.310501Z",
     "iopub.status.idle": "2024-05-21T11:44:02.348948Z",
     "shell.execute_reply": "2024-05-21T11:44:02.347458Z"
    },
    "papermill": {
     "duration": 0.051063,
     "end_time": "2024-05-21T11:44:02.351661",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.300598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: [0.95 0.95 0.95 0.95 0.95]\n",
      "Mean accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic dataset with groups\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "groups = np.array([i // 10 for i in range(100)])  # Create 10 groups\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define Group K-Fold Cross Validator\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=gkf, groups=groups, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68baf363",
   "metadata": {
    "papermill": {
     "duration": 0.007859,
     "end_time": "2024-05-21T11:44:02.367704",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.359845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Stratified Group K-Fold Cross Validation\n",
    "\n",
    "Stratified Group K-Fold Cross Validation is a variation of Group K-Fold Cross Validation that maintains the proportion of each class within each fold, while also ensuring that groups are not split across training and validation sets. This method is particularly useful for imbalanced datasets organized into groups, where it is important to preserve the class distribution within each fold.\n",
    "\n",
    "#### How Stratified Group K-Fold Cross Validation Works\n",
    "1. **Identify Groups**: Identify the groups within the dataset.\n",
    "2. **Divide Groups**: Split the groups into `k` folds while maintaining the class distribution in each fold and ensuring that all data points within a group remain in the same fold.\n",
    "3. **Train and Validate**: For each fold:\n",
    "   - Train the model on `k-1` folds.\n",
    "   - Validate the model on the remaining fold.\n",
    "4. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n",
    "5. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n",
    "\n",
    "### When to Use Stratified Group K-Fold Cross Validation\n",
    "- **Imbalanced Datasets**: Particularly useful for imbalanced datasets to ensure each fold has a representative class distribution.\n",
    "- **Grouped Data**: When the dataset contains groups of data points that should not be split across training and validation sets to prevent data leakage.\n",
    "- **Classification Problems**: Ensures that the class distribution is preserved, leading to more reliable evaluation metrics.\n",
    "\n",
    "### Using Stratified Group K-Fold Cross Validation in Scikit-Learn\n",
    "\n",
    "Currently, Scikit-learn does not provide a built-in `StratifiedGroupKFold` class. However, you can create a custom implementation using a combination of `GroupKFold` and stratification logic. Here's an example implementation:\n",
    "\n",
    "### Key Parameters\n",
    "- `n_splits`: Number of folds.\n",
    "- `groups`: Array-like structure containing group labels for the samples.\n",
    "\n",
    "### Advantages\n",
    "- **Class Distribution Preservation**: Maintains the class distribution in each fold, leading to more reliable and representative performance metrics.\n",
    "- **Prevents Data Leakage**: Ensures that data from the same group does not appear in both training and validation sets, preventing data leakage.\n",
    "- **Appropriate for Grouped Data**: Suitable for datasets with natural groupings, such as clustered data or repeated measures.\n",
    "\n",
    "### Disadvantages\n",
    "- **Complex Implementation**: Requires custom implementation, as it is not natively supported by Scikit-learn.\n",
    "- **Computational Cost**: Slightly more computationally intensive than standard Group K-Fold Cross Validation due to the need to maintain class distribution.\n",
    "\n",
    "Stratified Group K-Fold Cross Validation is a powerful tool for evaluating machine learning models on imbalanced and grouped data. It combines the benefits of stratification and group handling, ensuring reliable and representative performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b386d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:44:02.386447Z",
     "iopub.status.busy": "2024-05-21T11:44:02.386034Z",
     "iopub.status.idle": "2024-05-21T11:44:02.434749Z",
     "shell.execute_reply": "2024-05-21T11:44:02.432881Z"
    },
    "papermill": {
     "duration": 0.061409,
     "end_time": "2024-05-21T11:44:02.437414",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.376005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for each fold: [0.85714286 0.77272727 1.         0.9047619  0.73684211]\n",
      "Mean accuracy: 0.8542948279790383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic dataset with groups\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=5, random_state=42)\n",
    "groups = np.random.randint(0, 10, size=len(y))  # Create 10 groups\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Define Stratified Group K-Fold Cross Validator\n",
    "sgkf = StratifiedGroupKFold(n_splits=5)\n",
    "\n",
    "# Perform Cross Validation\n",
    "scores = cross_val_score(model, X, y, cv=sgkf, groups=groups, scoring='accuracy')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56c9e3",
   "metadata": {
    "papermill": {
     "duration": 0.008469,
     "end_time": "2024-05-21T11:44:02.453927",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.445458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AutoCV\n",
    "\n",
    "AutoCV is an automated cross-validation framework that I am developing to simplify and streamline the process of cross-validation in machine learning projects. Built on top of scikit-learn, it aims to reduce the manual effort involved in evaluating machine learning models by providing an easy-to-use interface and a set of tools that automate various cross-validation tasks. While AutoCV is still in development and may have some problems, it holds significant potential for improvement and expansion.\n",
    "\n",
    "[Github](https://github.com/0xkerem/AutoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc7c1da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:44:02.474305Z",
     "iopub.status.busy": "2024-05-21T11:44:02.473842Z",
     "iopub.status.idle": "2024-05-21T11:44:04.287065Z",
     "shell.execute_reply": "2024-05-21T11:44:04.285486Z"
    },
    "papermill": {
     "duration": 1.826914,
     "end_time": "2024-05-21T11:44:04.290097",
     "exception": false,
     "start_time": "2024-05-21T11:44:02.463183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AutoCV'...\r\n",
      "remote: Enumerating objects: 152, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (152/152), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (112/112), done.\u001b[K\r\n",
      "remote: Total 152 (delta 85), reused 104 (delta 37), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (152/152), 27.43 KiB | 6.86 MiB/s, done.\r\n",
      "Resolving deltas: 100% (85/85), done.\r\n"
     ]
    }
   ],
   "source": [
    "# Clone the AutoCV repository\n",
    "!git clone https://github.com/0xkerem/AutoCV.git\n",
    "    \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow info and warnings\n",
    "\n",
    "# Import necessary libraries\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/AutoCV')  # Add AutoCV to the Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a241f072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:44:04.310547Z",
     "iopub.status.busy": "2024-05-21T11:44:04.310091Z",
     "iopub.status.idle": "2024-05-21T11:50:18.199547Z",
     "shell.execute_reply": "2024-05-21T11:50:18.198210Z"
    },
    "papermill": {
     "duration": 373.911053,
     "end_time": "2024-05-21T11:50:18.210466",
     "exception": false,
     "start_time": "2024-05-21T11:44:04.299413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 11:44:11.086331: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 11:44:11.086494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 11:44:11.263914: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: {'average_test_accuracy': 0.965413870246085, 'average_test_precision': 0.9747352721849366, 'average_test_recall': 0.9752945563012676, 'average_test_f1_score': 0.954213273676361}\n",
      "Cross-Validation Summary:\n",
      "-------------------------\n",
      "Model: LogisticRegression(max_iter=200)\n",
      "Cross-Validation Strategy: LeavePOut(p=2)\n",
      "Average Fit Time: 0.0252 seconds\n",
      "Average Score Time: 0.0059 seconds\n",
      "Scores:\n",
      "  average_test_accuracy: 0.9654\n",
      "  average_test_precision: 0.9747\n",
      "  average_test_recall: 0.9753\n",
      "  average_test_f1_score: 0.9542\n"
     ]
    }
   ],
   "source": [
    "from autocv import AutoCV  # Import the AutoCV module\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize the LogisticRegression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Initialize the AutoCV object\n",
    "auto_cv = AutoCV(model=model)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = auto_cv.cross_validate(X, y)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# Print summary of the AutoCV results\n",
    "auto_cv.summary()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 680.640485,
   "end_time": "2024-05-21T11:50:20.703062",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-21T11:39:00.062577",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
