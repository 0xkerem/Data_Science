{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is Cross Validation?\n\nCross-validation is a statistical method used to estimate the skill of machine learning models. It involves partitioning a dataset into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). This process is repeated multiple times (folds) to ensure that the model's performance is consistent and not due to random chance.\n\n## Why Do We Need Cross Validation?\n\n1. **Model Evaluation**: It provides a more reliable estimate of a model's performance than a single train/test split, especially on small datasets.\n   \n2. **Model Selection**: Helps in selecting the best model among different types by comparing their performance.\n\n3. **Parameter Tuning**: Assists in tuning hyperparameters of a model to achieve the best performance.\n\n4. **Detect Overfitting**: It helps to identify if the model is overfitting to the training data and performs poorly on unseen data.\n\n5. **Bias-Variance Tradeoff**: It provides a balance between bias and variance, leading to a more generalized model.\n\n## When Do We Need Cross Validation?\n\n1. **Small Datasets**: When the dataset is not large enough, cross-validation maximizes the amount of data used for training and testing.\n   \n2. **Model Comparison**: When you need to compare multiple machine learning models to find the best one for your data.\n\n3. **Hyperparameter Tuning**: During the process of finding the optimal hyperparameters for a model.\n\n4. **Generalization**: When you want to ensure that your model generalizes well to unseen data.\n\n## When Do We Not Need Cross Validation?\n\n1. **Large Datasets**: When the dataset is large enough that a single train/test split can give a reliable estimate of model performance.\n   \n2. **Real-Time Predictions**: In real-time or streaming applications where splitting data into folds and training multiple models is impractical due to time constraints.\n\n3. **Data Leakage**: If there's a risk of data leakage between folds, which can happen if the data is not properly randomized or if there's temporal dependency.\n\n4. **Simplistic Models**: When using very simple models or in cases where the problem is straightforward and does not require extensive validation.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Types of Cross Validation in Scikit-Learn\nCross-validation is a key technique for assessing the performance and robustness of a machine learning model. **Scikit-learn** offers several methods for cross-validation, each suited to different types of data and use cases. Here is a detailed overview of the various cross-validation techniques provided by scikit-learn:\n\n## 1. K-Fold Cross Validation\n\nK-Fold Cross Validation is a robust method used to evaluate the performance of a machine learning model. It involves dividing the dataset into `k` equal-sized subsets or \"folds.\" The model is trained `k` times, each time using a different fold as the validation set and the remaining folds as the training set. The final performance metric is the average of the metrics calculated for each fold. This method helps to ensure that the model's performance is not dependent on the specific partitioning of the dataset.\n\n#### How K-Fold Cross Validation Works\n1. **Divide the Data**: Split the data into `k` equal-sized folds.\n2. **Train and Validate**: For each fold:\n   - Train the model on `k-1` folds.\n   - Validate the model on the remaining fold.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n4. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n\n![K-Fold Cross Validation source: Towards Data Science](https://i.ibb.co/H7mLgkk/k-fold-cv.png)\n\n### When to Use K-Fold Cross Validation\n- **Small Datasets**: When you have limited data, K-Fold Cross Validation allows you to make the most of your data by ensuring each data point is used for both training and validation.\n- **Model Selection**: When selecting the best model from a set of candidate models, K-Fold Cross Validation provides a reliable estimate of each modelâ€™s performance.\n- **Hyperparameter Tuning**: It helps in tuning hyperparameters by providing a robust evaluation metric.\n- **Avoiding Overfitting**: It helps in detecting overfitting by ensuring that the model performs well on different subsets of the data.\n\n### Using K-Fold Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement K-Fold Cross Validation using the `KFold` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of folds.\n- `shuffle`: Whether to shuffle the data before splitting into folds.\n- `random_state`: Seed for random number generator to ensure reproducibility.\n\n### Advantages\n- **Robust Evaluation**: Provides a more reliable estimate of model performance.\n- **Efficient Use of Data**: Makes efficient use of limited data by utilizing every observation for both training and validation.\n- **Reduced Variance**: Reduces the variance of performance metrics compared to a single train-test split.\n\n### Disadvantages\n- **Computational Cost**: More computationally expensive than a single train-test split, especially for large datasets.\n- **Model Complexity**: May lead to more complex models due to the multiple training processes.\n\nBy using K-Fold Cross Validation, you can ensure a thorough and reliable evaluation of your machine learning models, leading to better model selection and improved generalization to unseen data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define K-Fold Cross Validator\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Stratified K-Fold Cross Validation\n\nStratified K-Fold Cross Validation is a variation of K-Fold Cross Validation that ensures each fold is representative of the entire dataset by maintaining the same proportion of each class label. This method is particularly useful when dealing with imbalanced datasets, where some classes are underrepresented.\n\n#### How Stratified K-Fold Cross Validation Works\n1. **Divide the Data**: Split the data into `k` equal-sized folds while preserving the class distribution in each fold.\n2. **Train and Validate**: For each fold:\n   - Train the model on `k-1` folds.\n   - Validate the model on the remaining fold.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n4. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n\n### When to Use Stratified K-Fold Cross Validation\n- **Imbalanced Datasets**: When dealing with imbalanced datasets, Stratified K-Fold Cross Validation ensures that each fold is representative of the overall class distribution, providing a more reliable evaluation.\n- **Classification Problems**: Especially useful in classification problems where maintaining the class distribution in training and validation sets is crucial for model performance.\n- **Model Selection and Hyperparameter Tuning**: Helps in selecting the best model and tuning hyperparameters by providing a robust evaluation metric that accounts for class imbalance.\n\n### Using Stratified K-Fold Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement Stratified K-Fold Cross Validation using the `StratifiedKFold` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of folds.\n- `shuffle`: Whether to shuffle the data before splitting into folds.\n- `random_state`: Seed for random number generator to ensure reproducibility.\n\n### Advantages\n- **Class Distribution Preservation**: Maintains the proportion of each class in all folds, leading to more reliable performance metrics.\n- **Effective for Imbalanced Data**: Provides a better evaluation for models trained on imbalanced datasets.\n- **Reduced Bias**: Reduces bias in performance metrics by ensuring each fold is representative of the entire dataset.\n\n### Disadvantages\n- **Computational Cost**: More computationally expensive than a single train-test split, especially for large datasets.\n- **Complexity in Implementation**: Slightly more complex to implement compared to standard K-Fold Cross Validation, although libraries like Scikit-learn simplify this process.\n\nStratified K-Fold Cross Validation is a powerful tool for evaluating machine learning models, particularly when dealing with imbalanced datasets. It ensures that each fold is representative of the overall class distribution, leading to more reliable and robust performance metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Stratified K-Fold Cross Validator\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Leave-One-Out Cross Validation (LOOCV)\n\nLeave-One-Out Cross Validation (LOOCV) is an extreme case of K-Fold Cross Validation where `k` equals the number of data points in the dataset. In LOOCV, each data point is used once as a validation set while the remaining data points form the training set. This process is repeated for each data point, and the performance metric is averaged across all iterations.\n\n#### How Leave-One-Out Cross Validation Works\n1. **Divide the Data**: Treat each data point as a single fold.\n2. **Train and Validate**: For each data point:\n   - Train the model on the remaining `n-1` data points.\n   - Validate the model on the single data point.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each iteration.\n4. **Average Metrics**: Average the performance metrics across all iterations to obtain a final evaluation.\n\n### When to Use Leave-One-Out Cross Validation\n- **Small Datasets**: Ideal for very small datasets where splitting the data into larger folds is not feasible.\n- **High-Variance Models**: Useful when you want to get an unbiased estimate of model performance, though it may have high variance.\n- **Model Evaluation**: Provides a thorough evaluation as each data point is used for validation exactly once.\n\n### Using Leave-One-Out Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement LOOCV using the `LeaveOneOut` class and `cross_val_score` function.\n\n### Key Parameters\n- `cv`: Number of folds, which in the case of LOOCV is equal to the number of data points in the dataset.\n\n### Advantages\n- **Unbiased Estimate**: Provides an unbiased estimate of the modelâ€™s performance since each data point is used for validation exactly once.\n- **Maximal Data Utilization**: Ensures maximal utilization of data for training since `n-1` data points are used for training in each iteration.\n\n### Disadvantages\n- **Computationally Intensive**: Very computationally expensive, especially for large datasets, as it requires training the model `n` times.\n- **High Variance**: The performance metric can have high variance since each validation set contains only one data point.\n\nLeave-One-Out Cross Validation is a thorough and unbiased method for model evaluation, particularly useful for small datasets. However, its high computational cost makes it impractical for larger datasets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import LeaveOneOut, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Leave-One-Out Cross Validator\nloo = LeaveOneOut()\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=loo, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Leave-P-Out Cross Validation (LPOCV)\n\nLeave-P-Out Cross Validation (LPOCV) is a generalization of Leave-One-Out Cross Validation (LOOCV). In LPOCV, `p` data points are left out for validation, and the model is trained on the remaining `n-p` data points. This process is repeated for all possible combinations of `p` data points. LPOCV provides a comprehensive evaluation of the model's performance but is computationally expensive, especially for large `p` and datasets.\n\n#### How Leave-P-Out Cross Validation Works\n1. **Divide the Data**: Generate all possible combinations of `p` data points to be used as validation sets.\n2. **Train and Validate**: For each combination:\n   - Train the model on the remaining `n-p` data points.\n   - Validate the model on the `p` data points.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each iteration.\n4. **Average Metrics**: Average the performance metrics across all iterations to obtain a final evaluation.\n\n### When to Use Leave-P-Out Cross Validation\n- **Small to Medium Datasets**: Feasible for smaller datasets where the number of combinations is manageable.\n- **Thorough Evaluation**: Provides a thorough and exhaustive evaluation of model performance by considering all possible validation sets of size `p`.\n- **Model Evaluation**: Useful for understanding model performance across different subsets of the data.\n\n### Using Leave-P-Out Cross Validation in Scikit-Learn\n\nScikit-learn provides a way to implement LPOCV using the `LeavePOut` class and `cross_val_score` function.\n\n### Key Parameters\n- `p`: Number of data points to leave out for validation.\n- `cv`: Number of combinations, which is determined by the number of ways to choose `p` data points from `n`.\n\n### Advantages\n- **Comprehensive Evaluation**: Provides a thorough and exhaustive evaluation of the model by considering all possible subsets of size `p`.\n- **Detailed Insight**: Offers detailed insight into model performance across different combinations of data points.\n\n### Disadvantages\n- **Computationally Intensive**: Very computationally expensive, especially for large datasets and larger values of `p`, due to the combinatorial explosion of possible subsets.\n- **High Complexity**: High complexity in implementation and computation makes it impractical for large datasets or large `p`.\n\nLeave-P-Out Cross Validation is a powerful tool for thorough model evaluation, especially useful for smaller datasets where an exhaustive assessment is feasible. Its computational intensity makes it less suitable for large datasets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import LeavePOut, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Leave-P-Out Cross Validator\nlpo = LeavePOut(p=2)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=lpo, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}