{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is Cross Validation?\n\nCross-validation is a statistical method used to estimate the skill of machine learning models. It involves partitioning a dataset into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). This process is repeated multiple times (folds) to ensure that the model's performance is consistent and not due to random chance.\n\n## Why Do We Need Cross Validation?\n\n1. **Model Evaluation**: It provides a more reliable estimate of a model's performance than a single train/test split, especially on small datasets.\n   \n2. **Model Selection**: Helps in selecting the best model among different types by comparing their performance.\n\n3. **Parameter Tuning**: Assists in tuning hyperparameters of a model to achieve the best performance.\n\n4. **Detect Overfitting**: It helps to identify if the model is overfitting to the training data and performs poorly on unseen data.\n\n5. **Bias-Variance Tradeoff**: It provides a balance between bias and variance, leading to a more generalized model.\n\n## When Do We Need Cross Validation?\n\n1. **Small Datasets**: When the dataset is not large enough, cross-validation maximizes the amount of data used for training and testing.\n   \n2. **Model Comparison**: When you need to compare multiple machine learning models to find the best one for your data.\n\n3. **Hyperparameter Tuning**: During the process of finding the optimal hyperparameters for a model.\n\n4. **Generalization**: When you want to ensure that your model generalizes well to unseen data.\n\n## When Do We Not Need Cross Validation?\n\n1. **Large Datasets**: When the dataset is large enough that a single train/test split can give a reliable estimate of model performance.\n   \n2. **Real-Time Predictions**: In real-time or streaming applications where splitting data into folds and training multiple models is impractical due to time constraints.\n\n3. **Data Leakage**: If there's a risk of data leakage between folds, which can happen if the data is not properly randomized or if there's temporal dependency.\n\n4. **Simplistic Models**: When using very simple models or in cases where the problem is straightforward and does not require extensive validation.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Types of Cross Validation in Scikit-Learn\nCross-validation is a key technique for assessing the performance and robustness of a machine learning model. **Scikit-learn** offers several methods for cross-validation, each suited to different types of data and use cases. Here is a detailed overview of the various cross-validation techniques provided by scikit-learn:\n\n## 1. K-Fold Cross Validation\n\nK-Fold Cross Validation is a robust method used to evaluate the performance of a machine learning model. It involves dividing the dataset into `k` equal-sized subsets or \"folds.\" The model is trained `k` times, each time using a different fold as the validation set and the remaining folds as the training set. The final performance metric is the average of the metrics calculated for each fold. This method helps to ensure that the model's performance is not dependent on the specific partitioning of the dataset.\n\n#### How K-Fold Cross Validation Works\n1. **Divide the Data**: Split the data into `k` equal-sized folds.\n2. **Train and Validate**: For each fold:\n   - Train the model on `k-1` folds.\n   - Validate the model on the remaining fold.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n4. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n\n![K-Fold Cross Validation source: Towards Data Science](https://i.ibb.co/H7mLgkk/k-fold-cv.png)\n\n### When to Use K-Fold Cross Validation\n- **Small Datasets**: When you have limited data, K-Fold Cross Validation allows you to make the most of your data by ensuring each data point is used for both training and validation.\n- **Model Selection**: When selecting the best model from a set of candidate models, K-Fold Cross Validation provides a reliable estimate of each model’s performance.\n- **Hyperparameter Tuning**: It helps in tuning hyperparameters by providing a robust evaluation metric.\n- **Avoiding Overfitting**: It helps in detecting overfitting by ensuring that the model performs well on different subsets of the data.\n\n### Using K-Fold Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement K-Fold Cross Validation using the `KFold` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of folds.\n- `shuffle`: Whether to shuffle the data before splitting into folds.\n- `random_state`: Seed for random number generator to ensure reproducibility.\n\n### Advantages\n- **Robust Evaluation**: Provides a more reliable estimate of model performance.\n- **Efficient Use of Data**: Makes efficient use of limited data by utilizing every observation for both training and validation.\n- **Reduced Variance**: Reduces the variance of performance metrics compared to a single train-test split.\n\n### Disadvantages\n- **Computational Cost**: More computationally expensive than a single train-test split, especially for large datasets.\n- **Model Complexity**: May lead to more complex models due to the multiple training processes.\n\nBy using K-Fold Cross Validation, you can ensure a thorough and reliable evaluation of your machine learning models, leading to better model selection and improved generalization to unseen data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define K-Fold Cross Validator\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Stratified K-Fold Cross Validation\n\nStratified K-Fold Cross Validation is a variation of K-Fold Cross Validation that ensures each fold is representative of the entire dataset by maintaining the same proportion of each class label. This method is particularly useful when dealing with imbalanced datasets, where some classes are underrepresented.\n\n#### How Stratified K-Fold Cross Validation Works\n1. **Divide the Data**: Split the data into `k` equal-sized folds while preserving the class distribution in each fold.\n2. **Train and Validate**: For each fold:\n   - Train the model on `k-1` folds.\n   - Validate the model on the remaining fold.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n4. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n\n### When to Use Stratified K-Fold Cross Validation\n- **Imbalanced Datasets**: When dealing with imbalanced datasets, Stratified K-Fold Cross Validation ensures that each fold is representative of the overall class distribution, providing a more reliable evaluation.\n- **Classification Problems**: Especially useful in classification problems where maintaining the class distribution in training and validation sets is crucial for model performance.\n- **Model Selection and Hyperparameter Tuning**: Helps in selecting the best model and tuning hyperparameters by providing a robust evaluation metric that accounts for class imbalance.\n\n### Using Stratified K-Fold Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement Stratified K-Fold Cross Validation using the `StratifiedKFold` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of folds.\n- `shuffle`: Whether to shuffle the data before splitting into folds.\n- `random_state`: Seed for random number generator to ensure reproducibility.\n\n### Advantages\n- **Class Distribution Preservation**: Maintains the proportion of each class in all folds, leading to more reliable performance metrics.\n- **Effective for Imbalanced Data**: Provides a better evaluation for models trained on imbalanced datasets.\n- **Reduced Bias**: Reduces bias in performance metrics by ensuring each fold is representative of the entire dataset.\n\n### Disadvantages\n- **Computational Cost**: More computationally expensive than a single train-test split, especially for large datasets.\n- **Complexity in Implementation**: Slightly more complex to implement compared to standard K-Fold Cross Validation, although libraries like Scikit-learn simplify this process.\n\nStratified K-Fold Cross Validation is a powerful tool for evaluating machine learning models, particularly when dealing with imbalanced datasets. It ensures that each fold is representative of the overall class distribution, leading to more reliable and robust performance metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Stratified K-Fold Cross Validator\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Leave-One-Out Cross Validation (LOOCV)\n\nLeave-One-Out Cross Validation (LOOCV) is an extreme case of K-Fold Cross Validation where `k` equals the number of data points in the dataset. In LOOCV, each data point is used once as a validation set while the remaining data points form the training set. This process is repeated for each data point, and the performance metric is averaged across all iterations.\n\n#### How Leave-One-Out Cross Validation Works\n1. **Divide the Data**: Treat each data point as a single fold.\n2. **Train and Validate**: For each data point:\n   - Train the model on the remaining `n-1` data points.\n   - Validate the model on the single data point.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each iteration.\n4. **Average Metrics**: Average the performance metrics across all iterations to obtain a final evaluation.\n\n### When to Use Leave-One-Out Cross Validation\n- **Small Datasets**: Ideal for very small datasets where splitting the data into larger folds is not feasible.\n- **High-Variance Models**: Useful when you want to get an unbiased estimate of model performance, though it may have high variance.\n- **Model Evaluation**: Provides a thorough evaluation as each data point is used for validation exactly once.\n\n### Using Leave-One-Out Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement LOOCV using the `LeaveOneOut` class and `cross_val_score` function.\n\n### Key Parameters\n- `cv`: Number of folds, which in the case of LOOCV is equal to the number of data points in the dataset.\n\n### Advantages\n- **Unbiased Estimate**: Provides an unbiased estimate of the model’s performance since each data point is used for validation exactly once.\n- **Maximal Data Utilization**: Ensures maximal utilization of data for training since `n-1` data points are used for training in each iteration.\n\n### Disadvantages\n- **Computationally Intensive**: Very computationally expensive, especially for large datasets, as it requires training the model `n` times.\n- **High Variance**: The performance metric can have high variance since each validation set contains only one data point.\n\nLeave-One-Out Cross Validation is a thorough and unbiased method for model evaluation, particularly useful for small datasets. However, its high computational cost makes it impractical for larger datasets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import LeaveOneOut, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Leave-One-Out Cross Validator\nloo = LeaveOneOut()\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=loo, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Leave-P-Out Cross Validation (LPOCV)\n\nLeave-P-Out Cross Validation (LPOCV) is a generalization of Leave-One-Out Cross Validation (LOOCV). In LPOCV, `p` data points are left out for validation, and the model is trained on the remaining `n-p` data points. This process is repeated for all possible combinations of `p` data points. LPOCV provides a comprehensive evaluation of the model's performance but is computationally expensive, especially for large `p` and datasets.\n\n#### How Leave-P-Out Cross Validation Works\n1. **Divide the Data**: Generate all possible combinations of `p` data points to be used as validation sets.\n2. **Train and Validate**: For each combination:\n   - Train the model on the remaining `n-p` data points.\n   - Validate the model on the `p` data points.\n3. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each iteration.\n4. **Average Metrics**: Average the performance metrics across all iterations to obtain a final evaluation.\n\n### When to Use Leave-P-Out Cross Validation\n- **Small to Medium Datasets**: Feasible for smaller datasets where the number of combinations is manageable.\n- **Thorough Evaluation**: Provides a thorough and exhaustive evaluation of model performance by considering all possible validation sets of size `p`.\n- **Model Evaluation**: Useful for understanding model performance across different subsets of the data.\n\n### Using Leave-P-Out Cross Validation in Scikit-Learn\n\nScikit-learn provides a way to implement LPOCV using the `LeavePOut` class and `cross_val_score` function.\n\n### Key Parameters\n- `p`: Number of data points to leave out for validation.\n- `cv`: Number of combinations, which is determined by the number of ways to choose `p` data points from `n`.\n\n### Advantages\n- **Comprehensive Evaluation**: Provides a thorough and exhaustive evaluation of the model by considering all possible subsets of size `p`.\n- **Detailed Insight**: Offers detailed insight into model performance across different combinations of data points.\n\n### Disadvantages\n- **Computationally Intensive**: Very computationally expensive, especially for large datasets and larger values of `p`, due to the combinatorial explosion of possible subsets.\n- **High Complexity**: High complexity in implementation and computation makes it impractical for large datasets or large `p`.\n\nLeave-P-Out Cross Validation is a powerful tool for thorough model evaluation, especially useful for smaller datasets where an exhaustive assessment is feasible. Its computational intensity makes it less suitable for large datasets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import LeavePOut, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Leave-P-Out Cross Validator\nlpo = LeavePOut(p=2)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=lpo, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. ShuffleSplit Cross Validation\n\nShuffleSplit Cross Validation is a method that involves randomly shuffling the dataset and then splitting it into a specified number of training and validation sets. Unlike K-Fold Cross Validation, ShuffleSplit does not ensure that each sample is used exactly once for validation. Instead, it allows for random sampling with replacement, which can be useful for generating multiple different training and validation splits.\n\n#### How ShuffleSplit Cross Validation Works\n1. **Shuffle the Data**: Randomly shuffle the dataset.\n2. **Split the Data**: Split the shuffled data into training and validation sets according to specified proportions.\n3. **Train and Validate**: For each split:\n   - Train the model on the training set.\n   - Validate the model on the validation set.\n4. **Repeat**: Repeat the process for a specified number of iterations.\n5. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each split.\n6. **Average Metrics**: Average the performance metrics across all splits to obtain a final evaluation.\n\n### When to Use ShuffleSplit Cross Validation\n- **Large Datasets**: Suitable for large datasets where traditional K-Fold Cross Validation may be computationally expensive.\n- **Random Sampling**: When you want to ensure that different random samples of the dataset are used for training and validation.\n- **Model Robustness**: Useful for testing the robustness of the model against different random splits of the data.\n\n### Using ShuffleSplit Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement ShuffleSplit Cross Validation using the `ShuffleSplit` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of re-shuffling and splitting iterations.\n- `test_size`: Proportion of the dataset to include in the validation set.\n- `train_size`: Proportion of the dataset to include in the training set (if not specified, it's the complement of `test_size`).\n- `random_state`: Seed for random number generator to ensure reproducibility.\n\n### Advantages\n- **Flexibility**: Allows for flexible training and validation sizes, making it adaptable to various dataset sizes and requirements.\n- **Random Sampling**: Provides different random splits of the data, which can help in assessing the robustness of the model.\n- **Computational Efficiency**: Can be more computationally efficient than K-Fold Cross Validation for large datasets.\n\n### Disadvantages\n- **Potential Overlap**: Some samples may be included in both training and validation sets across different splits, which may lead to less independent evaluation.\n- **Less Comprehensive**: Does not ensure that each sample is used exactly once for validation, potentially leading to less comprehensive evaluation compared to K-Fold Cross Validation.\n\nShuffleSplit Cross Validation is a versatile and flexible method for evaluating machine learning models, particularly useful for large datasets and scenarios where random sampling is desired. Its ability to provide multiple different training and validation splits can help in assessing the robustness and generalization capability of the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define ShuffleSplit Cross Validator\nss = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=ss, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each split: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Stratified ShuffleSplit Cross Validation\n\nStratified ShuffleSplit Cross Validation is a variation of ShuffleSplit Cross Validation that maintains the proportion of each class in both the training and validation sets. This method is particularly useful for imbalanced datasets where it is important to ensure that each split preserves the original class distribution.\n\n#### How Stratified ShuffleSplit Cross Validation Works\n1. **Shuffle the Data**: Randomly shuffle the dataset while maintaining class distribution.\n2. **Split the Data**: Split the shuffled data into training and validation sets according to specified proportions, ensuring each set retains the class distribution.\n3. **Train and Validate**: For each split:\n   - Train the model on the training set.\n   - Validate the model on the validation set.\n4. **Repeat**: Repeat the process for a specified number of iterations.\n5. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each split.\n6. **Average Metrics**: Average the performance metrics across all splits to obtain a final evaluation.\n\n### When to Use Stratified ShuffleSplit Cross Validation\n- **Imbalanced Datasets**: Particularly useful when dealing with imbalanced datasets to ensure each class is appropriately represented in training and validation sets.\n- **Classification Problems**: Ensures that the class distribution is preserved, leading to more reliable evaluation metrics.\n- **Model Robustness**: Useful for testing the robustness of the model against different random splits of the data while maintaining class proportions.\n\n### Using Stratified ShuffleSplit Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement Stratified ShuffleSplit Cross Validation using the `StratifiedShuffleSplit` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of re-shuffling and splitting iterations.\n- `test_size`: Proportion of the dataset to include in the validation set.\n- `train_size`: Proportion of the dataset to include in the training set (if not specified, it's the complement of `test_size`).\n- `random_state`: Seed for random number generator to ensure reproducibility.\n\n### Advantages\n- **Class Distribution Preservation**: Maintains the class distribution in each split, leading to more reliable and representative performance metrics.\n- **Random Sampling**: Provides different random splits of the data while preserving class proportions, useful for assessing model robustness.\n- **Flexible and Robust**: Offers the flexibility of ShuffleSplit with the added benefit of stratification, making it suitable for a wide range of dataset sizes and types.\n\n### Disadvantages\n- **Potential Overlap**: Some samples may be included in both training and validation sets across different splits, which may lead to less independent evaluation.\n- **Computational Cost**: Can be computationally intensive for very large datasets, though generally more efficient than exhaustive methods like LPOCV.\n\nStratified ShuffleSplit Cross Validation combines the benefits of random sampling with the need to maintain class distributions, making it a powerful tool for evaluating machine learning models, especially on imbalanced datasets. This method helps ensure that evaluation metrics are reliable and representative of the model's performance across different splits of the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\ndata = load_iris()\nX, y = data.data, data.target\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Stratified ShuffleSplit Cross Validator\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=sss, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each split: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Group K-Fold Cross Validation\n\nGroup K-Fold Cross Validation is a variation of K-Fold Cross Validation used when the data is organized into groups that should not be split across training and validation sets. It ensures that the same group is not represented in both training and validation sets, making it useful for scenarios where there is a need to avoid data leakage between groups, such as in time-series data, clustered data, or repeated measures.\n\n#### How Group K-Fold Cross Validation Works\n1. **Identify Groups**: Identify the groups within the dataset.\n2. **Divide Groups**: Split the groups into `k` folds while ensuring that all data points within a group remain in the same fold.\n3. **Train and Validate**: For each fold:\n   - Train the model on `k-1` folds.\n   - Validate the model on the remaining fold.\n4. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each fold.\n5. **Average Metrics**: Average the performance metrics across all folds to obtain a final evaluation.\n\n### When to Use Group K-Fold Cross Validation\n- **Grouped Data**: When the dataset contains groups of data points that should not be split across training and validation sets to prevent data leakage.\n- **Time-Series Data**: When working with time-series data or sequential data where the order and grouping of data points are crucial.\n- **Hierarchical Data**: When dealing with hierarchical data or repeated measures where individual samples are not independent.\n\n### Using Group K-Fold Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement Group K-Fold Cross Validation using the `GroupKFold` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of folds.\n- `groups`: Array-like structure containing group labels for the samples.\n\n### Advantages\n- **Prevents Data Leakage**: Ensures that data from the same group does not appear in both training and validation sets, preventing data leakage.\n- **Appropriate for Grouped Data**: Suitable for datasets with natural groupings, such as clustered data or repeated measures.\n- **Maintains Group Integrity**: Ensures the integrity of groups within the dataset, making it a robust evaluation method for grouped data.\n\n### Disadvantages\n- **Computational Cost**: Slightly more computationally intensive than standard K-Fold Cross Validation due to the need to maintain group integrity.\n- **Complexity in Implementation**: Requires additional handling of group labels and careful management of group splits.\n\nGroup K-Fold Cross Validation is a powerful tool for evaluating machine learning models when dealing with grouped data. It helps prevent data leakage and ensures that the evaluation metrics are reliable and representative of the model's performance across different groups in the dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold, cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Create a synthetic dataset with groups\nX, y = make_classification(n_samples=100, n_features=10, random_state=42)\ngroups = np.array([i // 10 for i in range(100)])  # Create 10 groups\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Group K-Fold Cross Validator\ngkf = GroupKFold(n_splits=5)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=gkf, groups=groups, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each fold: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Group ShuffleSplit Cross Validation\n\nGroup ShuffleSplit Cross Validation is a variation of ShuffleSplit Cross Validation designed for datasets organized into groups. This method involves shuffling the groups and then splitting them into training and validation sets while ensuring that the same group does not appear in both sets. It provides a way to generate multiple random splits while maintaining group integrity, making it useful for scenarios where data is grouped and you want to avoid data leakage between groups.\n\n#### How Group ShuffleSplit Cross Validation Works\n1. **Identify Groups**: Identify the groups within the dataset.\n2. **Shuffle Groups**: Randomly shuffle the groups.\n3. **Split Groups**: Split the shuffled groups into training and validation sets according to specified proportions, ensuring each group remains intact.\n4. **Train and Validate**: For each split:\n   - Train the model on the training set.\n   - Validate the model on the validation set.\n5. **Repeat**: Repeat the process for a specified number of iterations.\n6. **Compute Metrics**: Calculate performance metrics (e.g., accuracy, precision, recall) for each split.\n7. **Average Metrics**: Average the performance metrics across all splits to obtain a final evaluation.\n\n### When to Use Group ShuffleSplit Cross Validation\n- **Grouped Data**: When the dataset contains groups of data points that should not be split across training and validation sets to prevent data leakage.\n- **Random Sampling with Groups**: When you want to ensure random sampling of groups for training and validation.\n- **Model Robustness**: Useful for testing the robustness of the model against different random splits of the data while maintaining group integrity.\n\n### Using Group ShuffleSplit Cross Validation in Scikit-Learn\n\nScikit-learn provides a convenient way to implement Group ShuffleSplit Cross Validation using the `GroupShuffleSplit` class and `cross_val_score` function.\n\n### Key Parameters\n- `n_splits`: Number of re-shuffling and splitting iterations.\n- `test_size`: Proportion of the dataset to include in the validation set.\n- `train_size`: Proportion of the dataset to include in the training set (if not specified, it's the complement of `test_size`).\n- `random_state`: Seed for random number generator to ensure reproducibility.\n- `groups`: Array-like structure containing group labels for the samples.\n\n### Advantages\n- **Prevents Data Leakage**: Ensures that data from the same group does not appear in both training and validation sets, preventing data leakage.\n- **Random Sampling with Groups**: Provides different random splits of the data while preserving group integrity, useful for assessing model robustness.\n- **Flexible and Robust**: Offers the flexibility of ShuffleSplit with the added benefit of group handling, making it suitable for various dataset sizes and types.\n\n### Disadvantages\n- **Potential Overlap**: Groups might be used in multiple splits, which can lead to less independent evaluation compared to Group K-Fold Cross Validation.\n- **Computational Cost**: Slightly more computationally intensive than standard ShuffleSplit due to the need to handle group splits.\n\nGroup ShuffleSplit Cross Validation is a versatile and flexible method for evaluating machine learning models on grouped data. It combines the benefits of random sampling with the need to maintain group integrity, ensuring reliable and representative performance metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupShuffleSplit, cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Create a synthetic dataset with groups\nX, y = make_classification(n_samples=100, n_features=10, random_state=42)\ngroups = np.array([i // 10 for i in range(100)])  # Create 10 groups\n\n# Initialize model\nmodel = LogisticRegression(max_iter=200)\n\n# Define Group ShuffleSplit Cross Validator\ngss = GroupShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\n# Perform Cross Validation\nscores = cross_val_score(model, X, y, cv=gss, groups=groups, scoring='accuracy')\n\n# Print results\nprint(f\"Accuracy scores for each split: {scores}\")\nprint(f\"Mean accuracy: {scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]}]}